Curriculum learning did not help.
It turns out that the hyperparameter tuning condition was not very good. So I have to do it manually. It might work.
Rather than worrying about backpropagation leaking, I should do the finaltest.py first. This requires some design, and
I need to look at the input to figure out.

The validation in tacotron seems arbitrarily low. There might be an issue.
The output of tacotron is very hard to read.

LSTM does not load correctly. lstm.cuda() returns CUDA errors.

Because we are doing manual tuning right now, it would be beneficial to spawn multiple instances of the model and
train with the same datastream. The reason for this is because data I/O seems to be the most time-consuming
pipeline. This means log will be the only reference of training. Given model size of ~2000mb, running 5~8 instances
should be reasonable.



DNC validation is at 0.0007. DNC running loss is around 0.0003. This is acceptable. I'm going to run with a slightly
larger parameter set.
Tacotron running at 0.0005. Validation method has high variance so I need something else.

Channeled LSTM performs poorly.