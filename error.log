22:11:23.642661Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 289, in main
    computer, optim, starting_epoch, starting_iteration = load_model(computer, optim, starting_epoch, starting_iteration)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 70, in load_model
    computer.load_state_dict(modelsd)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 508, in load_state_dict
    for name, param in state_dict.items():
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 398, in __getattr__
    type(self).__name__, name))
AttributeError: 'Frankenstein' object has no attribute 'items'
22:14:40.896984Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 289, in main
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 70, in load_model
    print('Loaded model at epoch ', highestepoch, 'iteartion', iteration)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 70, in load_model
    print('Loaded model at epoch ', highestepoch, 'iteartion', iteration)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 620, in trace_dispatch
    self.do_wait_suspend(thread, frame, event, arg)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 125, in do_wait_suspend
    self._args[0].do_wait_suspend(*args, **kwargs)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/pydevd.py", line 812, in do_wait_suspend
    time.sleep(0.01)
KeyboardInterrupt
22:17:23.132424Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 304, in main
    igdl, int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 226, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 99977) exited unexpectedly with exit code 1.
22:21:36.858445Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 304, in main
    igdl, int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 216, in train
    print_interval=10
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 216, in train
    print_interval=10
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 620, in trace_dispatch
    self.do_wait_suspend(thread, frame, event, arg)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 125, in do_wait_suspend
    self._args[0].do_wait_suspend(*args, **kwargs)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/pydevd.py", line 812, in do_wait_suspend
    time.sleep(0.01)
KeyboardInterrupt
22:23:21.142354Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 304, in main
    igdl, int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 226, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58
22:35:09.010267Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 304, in main
    igdl, int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 226, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 101044) exited unexpectedly with exit code 1.
22:37:57.200518Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 305, in main
    train(computer, optimizer, real_criterion, binary_criterion,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 226, in train
    train_story_loss = run_one_patient(computer, input, target, target_dim, optimizer, loss_type,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 170, in run_one_patient
    feeding = input[:, timestep, :]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 471, in forward
    read_modes)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 305, in read_weightings
    content_weighting = self.read_content_weighting(read_keys, read_strengths)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 137, in read_content_weighting
    innerprod = torch.matmul(self.memory.unsqueeze(0), read_keys)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/functional.py", line 212, in matmul
    tensor2_exp_size = torch.Size((1,) * max(3 - tensor2.dim(), 0) + tensor2.size())
KeyboardInterrupt
22:48:01.154252Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 307, in main
    igdl, int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 258, in train
    save_model(computer, optimizer, epoch, i)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 30, in save_model
    pickle_file = pickle_file.open('wb')
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/pathlib.py", line 1181, in open
    opener=self._opener)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/pathlib.py", line 1035, in _opener
    return self._accessor.open(self, flags, mode)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/pathlib.py", line 387, in wrapped
    return strfunc(str(pathobj), *args)
FileNotFoundError: [Errno 2] No such file or directory: '/local2/tmp/pycharm_project_292/death/DNC/saves/DNCfull_0_99.pkl'
01:54:12.799558Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 314, in main
    train(computer, optimizer, real_criterion, binary_criterion,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 263, in train
    if i % save_interval//10 == save_interval//10-1:
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 30, in save_model
    pickle_file = pickle_file.open('wb')
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/pathlib.py", line 1181, in open
    opener=self._opener)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/pathlib.py", line 1035, in _opener
    return self._accessor.open(self, flags, mode)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/pathlib.py", line 387, in wrapped
    return strfunc(str(pathobj), *args)
FileNotFoundError: [Errno 2] No such file or directory: '/local2/tmp/pycharm_project_292/death/DNC/saves/DNCfull_0_990.pkl'
12:29:03.466182Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 313, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 262, in train
    save_model(computer, optimizer, epoch, i)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 31, in save_model
    torch.save((net, optim, epoch, iteration), pickle_file)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 135, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 117, in _with_file_like
    return body(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 135, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 204, in _save
    serialized_storages[key]._write_file(f)
KeyboardInterrupt
12:38:06.199977Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 229, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 171, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 457, in forward
    allocation_weighting = self.allocation_weighting()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 209, in allocation_weighting
    cum_prod = torch.cat([Variable(torch.ones(self.bs, 1).cuda()), cum_prod], 1)[:, :-1]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/_utils.py", line 69, in _cuda
    return new_type(self.size()).copy_(self, async)
KeyboardInterrupt
12:48:08.000762Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 326, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 234, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 122067) exited unexpectedly with exit code 1.
12:55:26.838607Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 326, in main
    train(computer, optimizer, real_criterion, binary_criterion,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 234, in train
    train_story_loss = run_one_patient(computer, input, target, target_dim, optimizer, loss_type,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 122088) exited unexpectedly with exit code 1.
12:56:05.030753Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 327, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 243, in train
    (i, printloss))
TypeError: log_print() missing 1 required positional argument: 'logfile'
12:58:06.175431Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 327, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 235, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 122172) exited unexpectedly with exit code 1.
12:59:12.269230Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 327, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 235, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 122192) exited unexpectedly with exit code 1.
13:04:38.429404Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 328, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 235, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 171, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 457, in forward
    allocation_weighting = self.allocation_weighting()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 209, in allocation_weighting
    cum_prod = torch.cat([Variable(torch.ones(self.bs, 1).cuda()), cum_prod], 1)[:, :-1]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/_utils.py", line 69, in _cuda
    return new_type(self.size()).copy_(self, async)
KeyboardInterrupt
13:06:22.359980Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 298, in main
    logfile = "log.txt"
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 60, in __init__
    print("Input Gen initiated")
KeyboardInterrupt
13:06:32.227689Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 301, in main
    ig = InputGen()
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 45, in __init__
    self.load_pickle(verbose=verbose)
  File "/local2/tmp/pycharm_project_292/death/post/qdata.py", line 172, in load_pickle
    self.serv, self.surg, self.vital = pickle.load(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 169, in _new_Index
    def _new_Index(cls, d):
KeyboardInterrupt
13:08:09.438072Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 174, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 485, in forward
    raise ValueError("nan is found.")
ValueError: nan is found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 333, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 238, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 209, in run_one_patient
    global_exception_counter+=1
UnboundLocalError: local variable 'global_exception_counter' referenced before assignment
13:12:04.430964Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 204, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 333, in main
    train(computer, optimizer, real_criterion, binary_criterion,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 238, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 204, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 122679) exited unexpectedly with exit code 1.
13:15:41.838576Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 334, in main
    optimizer = torch.optim.Adam(computer.parameters(), lr=lr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 240, in train
    val_batch = 10
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 368, in new_sequence_reset
    RNN.new_sequence_reset()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 541, in new_sequence_reset
    self.W_forget.weight.detach()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 246, in detach
    result = NoGrad()(self)  # this is needed, because it merges version counters
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py", line 143, in _do_forward
    result = super(NoGrad, self)._do_forward(*args, **kwargs)
KeyboardInterrupt
13:27:52.086075Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 348, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 280, in train
    real_criterion, binary_criterion, validate=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 183, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 393, in forward
    hidden_previous_layer)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 523, in forward
    semicolon_input = torch.cat([input_x, previous_time, previous_layer], dim=1)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58
13:28:48.431772Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 316, in main
    ig = InputGen()
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 45, in __init__
    self.load_pickle(verbose=verbose)
  File "/local2/tmp/pycharm_project_292/death/post/qdata.py", line 172, in load_pickle
    self.serv, self.surg, self.vital = pickle.load(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 169, in _new_Index
    def _new_Index(cls, d):
KeyboardInterrupt
13:29:50.308481Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 345, in main
    traindl, iter(validdl), int(starting_epoch) , total_epochs, int(starting_iteration) , iter_per_epoch,
UnboundLocalError: local variable 'starting_epoch' referenced before assignment
13:32:05.407757Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 343, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 290, in train
    save_model(computer, optimizer, epoch, i)
KeyboardInterrupt
13:42:57.932729Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 345, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 282, in train
    real_criterion, binary_criterion, validate=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 185, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 393, in forward
    hidden_previous_layer)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 523, in forward
    semicolon_input = torch.cat([input_x, previous_time, previous_layer], dim=1)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58
13:45:56.302163Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 215, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 345, in main
    traindl, iter(validdl), int(starting_epoch) , total_epochs, int(starting_iteration) , iter_per_epoch,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 252, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 215, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 123979) exited unexpectedly with exit code 1.
13:51:27.223218Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 215, in run_one_patient
    global_exception_counter += 1
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 345, in main
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 252, in train
    if i % print_interval == 0:
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 215, in run_one_patient
    global_exception_counter += 1
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 124080) exited unexpectedly with exit code 1.
14:14:32.926727Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 389, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 291, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 210, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: element 0 of variables tuple is volatile
14:20:38.163644Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 389, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 291, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 210, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: element 0 of variables tuple is volatile
14:25:01.545305Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 391, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 327, in train
    printloss += float(val_loss[0])
UnboundLocalError: local variable 'val_loss' referenced before assignment
14:28:48.267991Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 391, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 292, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 210, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: element 0 of variables tuple is volatile
14:34:59.320245Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 210, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: element 0 of variables tuple is volatile

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 393, in main
    traindl, iter(validdl), int(starting_epoch) , total_epochs, int(starting_iteration) , iter_per_epoch,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 294, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 212, in run_one_patient
    raise
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 212, in run_one_patient
    raise
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 620, in trace_dispatch
    self.do_wait_suspend(thread, frame, event, arg)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 125, in do_wait_suspend
    self._args[0].do_wait_suspend(*args, **kwargs)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/pydevd.py", line 812, in do_wait_suspend
    time.sleep(0.01)
KeyboardInterrupt
14:39:21.755817Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 394, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 340, in train
    save_model(computer, optimizer, epoch, train_index)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 29, in save_model
    epoch = int(epoch)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 29, in save_model
    epoch = int(epoch)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 620, in trace_dispatch
    self.do_wait_suspend(thread, frame, event, arg)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 125, in do_wait_suspend
    self._args[0].do_wait_suspend(*args, **kwargs)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/pydevd.py", line 812, in do_wait_suspend
    time.sleep(0.01)
KeyboardInterrupt
14:57:48.130860Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 210, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: element 0 of variables tuple is volatile

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 394, in main
    train(computer, optimizer, real_criterion, binary_criterion,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 294, in train
    if train_index < iter_per_epoch:
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 212, in run_one_patient
    raise
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 212, in run_one_patient
    raise
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 620, in trace_dispatch
    self.do_wait_suspend(thread, frame, event, arg)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 125, in do_wait_suspend
    self._args[0].do_wait_suspend(*args, **kwargs)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/pydevd.py", line 812, in do_wait_suspend
    time.sleep(0.01)
KeyboardInterrupt
14:58:43.146775Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 396, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 297, in train
    printloss = float(train_story_loss[0])
TypeError: 'float' object is not subscriptable
15:02:22.120158Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 396, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 296, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 210, in run_one_patient
    patient_loss.backward()
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 406, in trace_dispatch
    def trace_dispatch(self, frame, event, arg):
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 129886) exited unexpectedly with exit code 1.
15:05:13.040267Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 210, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: element 0 of variables tuple is volatile

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 396, in main
    traindl, iter(validdl), int(starting_epoch) , total_epochs, int(starting_iteration) , iter_per_epoch,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 296, in train
    train_story_loss = run_one_patient(computer, input, target, target_dim, optimizer, loss_type,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 212, in run_one_patient
    traceback.print_exc()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 212, in run_one_patient
    traceback.print_exc()
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 620, in trace_dispatch
    self.do_wait_suspend(thread, frame, event, arg)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 125, in do_wait_suspend
    self._args[0].do_wait_suspend(*args, **kwargs)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/pydevd.py", line 812, in do_wait_suspend
    time.sleep(0.01)
KeyboardInterrupt
15:06:37.575768Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 397, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 343, in train
    save_model(computer, optimizer, epoch, train_index)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_trace_dispatch_regular.py", line 38, in trace_dispatch
    def trace_dispatch(py_db, frame, event, arg):
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 130855) exited unexpectedly with exit code 1.
15:06:54.067518Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/post/qdata.py", line 172, in load_pickle
    self.serv, self.surg, self.vital = pickle.load(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 169, in _new_Index
    def _new_Index(cls, d):
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_trace_dispatch_regular.py", line 84, in __call__
    def __call__(self, frame, event, arg):
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 366, in main
    ig = InputGen()
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 45, in __init__
    self.load_pickle(verbose=verbose)
  File "/local2/tmp/pycharm_project_292/death/post/qdata.py", line 172, in load_pickle
    self.serv, self.surg, self.vital = pickle.load(f)
KeyboardInterrupt
15:16:32.193535Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 210, in run_one_patient
    #     underestimation = nn.functional.relu(underestimation)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: element 0 of variables tuple is volatile

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 397, in main
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 297, in train
    val_batch = 1
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 213, in run_one_patient
    #     patient_loss = toe_loss/100 + cod_loss
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 213, in run_one_patient
    #     patient_loss = toe_loss/100 + cod_loss
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 620, in trace_dispatch
    self.do_wait_suspend(thread, frame, event, arg)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 125, in do_wait_suspend
    self._args[0].do_wait_suspend(*args, **kwargs)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/pydevd.py", line 812, in do_wait_suspend
    time.sleep(0.01)
KeyboardInterrupt
15:17:23.989987Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 405, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 307, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 173, in run_one_patient
    with torch.set_grad_enabled(True):
AttributeError: module 'torch' has no attribute 'set_grad_enabled'
15:54:32.054176Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 215, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: element 0 of variables tuple is volatile

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 403, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 305, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 218, in run_one_patient
    raise
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 218, in run_one_patient
    raise
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 620, in trace_dispatch
    self.do_wait_suspend(thread, frame, event, arg)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 125, in do_wait_suspend
    self._args[0].do_wait_suspend(*args, **kwargs)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/pydevd.py", line 812, in do_wait_suspend
    time.sleep(0.01)
KeyboardInterrupt
15:55:16.747711Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 390, in main
    optimizer = torch.optim.Adam(computer.parameters(), lr=lr)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/optim/adam.py", line 29, in __init__
    super(Adam, self).__init__(params, defaults)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/optim/optimizer.py", line 39, in __init__
    self.add_param_group(param_group)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/optim/optimizer.py", line 153, in add_param_group
    raise ValueError("optimizing a parameter that doesn't require gradients")
ValueError: optimizing a parameter that doesn't require gradients
15:57:59.512991Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 403, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 305, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 186, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 393, in forward
    hidden_previous_layer)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 532, in forward
    self.old_state = new_state
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 417, in __setattr__
    .format(torch.typename(value), name))
TypeError: cannot assign 'torch.autograd.variable.Variable' as parameter 'old_state' (torch.nn.Parameter or None expected)
16:12:21.979342Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 403, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 305, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 186, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 467, in forward
    self.update_temporal_linkage_matrix(write_weighting)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 266, in update_temporal_linkage_matrix
    ((1 - ww_j - ww_i) * batch_temporal_memory_linkage + ww_i * p_j).data,requires_grad=False)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 417, in __setattr__
    .format(torch.typename(value), name))
TypeError: cannot assign 'torch.autograd.variable.Variable' as parameter 'temporal_memory_linkage' (torch.nn.Parameter or None expected)
16:13:13.414766Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 403, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 305, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 186, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 474, in forward
    read_modes)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 322, in read_weightings
    self.last_read_weightings = Variable(read_weightings.data,requires_grad=False)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 417, in __setattr__
    .format(torch.typename(value), name))
TypeError: cannot assign 'torch.autograd.variable.Variable' as parameter 'last_read_weightings' (torch.nn.Parameter or None expected)
16:17:50.276144Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 403, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 305, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 186, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 460, in forward
    allocation_weighting = self.allocation_weighting()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 209, in allocation_weighting
    cum_prod = torch.cat([Variable(torch.ones(self.bs, 1).cuda()), cum_prod], 1)[:, :-1]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/_utils.py", line 69, in _cuda
    return new_type(self.size()).copy_(self, async)
KeyboardInterrupt
16:18:27.732318Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 372, in main
    ig = InputGen()
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 45, in __init__
    self.load_pickle(verbose=verbose)
  File "/local2/tmp/pycharm_project_292/death/post/qdata.py", line 172, in load_pickle
    self.serv, self.surg, self.vital = pickle.load(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 169, in _new_Index
    def _new_Index(cls, d):
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_trace_dispatch_regular.py", line 84, in __call__
    def __call__(self, frame, event, arg):
KeyboardInterrupt
16:38:18.923743Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 215, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 403, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 305, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 215, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 137704) exited unexpectedly with exit code 1.
17:03:43.432944Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 303, in main
    optimizer = torch.optim.Adam(computer.parameters(), lr=lr)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/optim/adam.py", line 29, in __init__
    super(Adam, self).__init__(params, defaults)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/optim/optimizer.py", line 39, in __init__
    self.add_param_group(param_group)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/optim/optimizer.py", line 153, in add_param_group
    raise ValueError("optimizing a parameter that doesn't require gradients")
ValueError: optimizing a parameter that doesn't require gradients
17:07:01.971087Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 303, in main
    optimizer = torch.optim.Adam(computer.parameters(), lr=lr)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/optim/adam.py", line 29, in __init__
    super(Adam, self).__init__(params, defaults)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/optim/optimizer.py", line 39, in __init__
    self.add_param_group(param_group)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/optim/optimizer.py", line 153, in add_param_group
    raise ValueError("optimizing a parameter that doesn't require gradients")
ValueError: optimizing a parameter that doesn't require gradients
17:12:43.502157Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 303, in main
    optimizer = torch.optim.Adam(computer.parameters(), lr=lr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 303, in main
    optimizer = torch.optim.Adam(computer.parameters(), lr=lr)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 620, in trace_dispatch
    self.do_wait_suspend(thread, frame, event, arg)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 125, in do_wait_suspend
    self._args[0].do_wait_suspend(*args, **kwargs)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/pydevd.py", line 812, in do_wait_suspend
    time.sleep(0.01)
KeyboardInterrupt
17:17:06.812831Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 303, in main
    optimizer = torch.optim.Adam(computer.parameters(), lr=lr)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/optim/adam.py", line 29, in __init__
    super(Adam, self).__init__(params, defaults)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/optim/optimizer.py", line 39, in __init__
    self.add_param_group(param_group)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/optim/optimizer.py", line 153, in add_param_group
    raise ValueError("optimizing a parameter that doesn't require gradients")
ValueError: optimizing a parameter that doesn't require gradients
17:41:43.309106Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 314, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 229, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 141897) exited unexpectedly with exit code 1.
12:07:27.523537Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 314, in main
    ig = InputGen()
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 45, in __init__
    self.load_pickle(verbose=verbose)
  File "/local2/tmp/pycharm_project_292/death/post/qdata.py", line 172, in load_pickle
    self.serv, self.surg, self.vital = pickle.load(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 169, in _new_Index
    def _new_Index(cls, d):
KeyboardInterrupt
12:16:22.379709Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 180, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 485, in forward
    raise ValueError("nan is found.")
ValueError: nan is found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 344, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 249, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 216, in run_one_patient
    global_exception_counter += 1
UnboundLocalError: local variable 'global_exception_counter' referenced before assignment
13:51:59.378566Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 344, in main
    traindl, iter(validdl), int(starting_epoch) , total_epochs, int(starting_iteration) , iter_per_epoch,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 291, in train
    if i == 0:
KeyboardInterrupt
13:55:41.529108Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 345, in main
    traindl, iter(validdl), int(starting_epoch) , total_epochs, int(starting_iteration) , iter_per_epoch,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 250, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 181, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 457, in forward
    allocation_weighting = self.allocation_weighting()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 209, in allocation_weighting
    cum_prod = torch.cat([Variable(torch.ones(self.bs, 1).cuda()), cum_prod], 1)[:, :-1]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/_utils.py", line 69, in _cuda
    return new_type(self.size()).copy_(self, async)
KeyboardInterrupt
13:58:20.355675Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 346, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 281, in train
    real_criterion, binary_criterion, validate=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 181, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 393, in forward
    hidden_previous_layer)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 523, in forward
    semicolon_input = torch.cat([input_x, previous_time, previous_layer], dim=1)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58
14:10:30.236666Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 347, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 282, in train
    real_criterion, binary_criterion, validate=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 182, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 393, in forward
    hidden_previous_layer)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 523, in forward
    semicolon_input = torch.cat([input_x, previous_time, previous_layer], dim=1)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58
14:12:24.845275Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 349, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 253, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 224, in run_one_patient
    del input, target, time_length, patient_output, timestep, feeding, output, patient_output, \
UnboundLocalError: local variable 'patient_output' referenced before assignment
14:13:39.803041Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 350, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 254, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 216, in run_one_patient
    del input, target, time_length, patient_output, timestep, feeding, output, patient_output, \
UnboundLocalError: local variable 'patient_output' referenced before assignment
14:29:54.974346Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 350, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 285, in train
    real_criterion, binary_criterion, validate=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 182, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 393, in forward
    hidden_previous_layer)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 523, in forward
    semicolon_input = torch.cat([input_x, previous_time, previous_layer], dim=1)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58
14:34:24.925582Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 350, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 285, in train
    real_criterion, binary_criterion, validate=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 182, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 393, in forward
    hidden_previous_layer)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 523, in forward
    semicolon_input = torch.cat([input_x, previous_time, previous_layer], dim=1)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58
14:40:56.586796Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 353, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 288, in train
    real_criterion, binary_criterion, validate=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 184, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 393, in forward
    hidden_previous_layer)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 523, in forward
    semicolon_input = torch.cat([input_x, previous_time, previous_layer], dim=1)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58
15:52:02.686108Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 442, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 388, in train
    save_model(computer, optimizer, epoch, i)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 40, in save_model
    torch.save((net, optim, epoch, iteration), pickle_file)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 135, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 117, in _with_file_like
    return body(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 135, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 204, in _save
    serialized_storages[key]._write_file(f)
KeyboardInterrupt
15:53:19.449741Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 186, in validate_batch_patients
    (input, target, loss_type) = next(valid_iterator)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 259, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 259, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 363, in __getitem__
    return self.mother[self.mapping[index]]
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 237, in __getitem__
    allrows = df.loc[[id]]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexing.py", line 1478, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexing.py", line 1901, in _getitem_axis
    return self._getitem_iterable(key, axis=axis)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexing.py", line 1134, in _getitem_iterable
    key, kind=self.name)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 1995, in _convert_listlike_indexer
    _, indexer = self.reindex(keyarr, level=level)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 2084, in reindex
    keep_order=False)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 3955, in _join_level
    new_lev_labels, ngroups)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 442, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 370, in train
    real_criterion, binary_criterion, val_batch)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 186, in validate_batch_patients
    (input, target, loss_type) = next(valid_iterator)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 180218) exited unexpectedly with exit code 1.
16:06:50.321339Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 293, in train_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 442, in main
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 335, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 293, in train_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 180423) exited unexpectedly with exit code 1.
16:06:59.434271Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 414, in main
    ig = InputGen()
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 45, in __init__
    self.load_pickle(verbose=verbose)
  File "/local2/tmp/pycharm_project_292/death/post/qdata.py", line 172, in load_pickle
    self.serv, self.surg, self.vital = pickle.load(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 169, in _new_Index
    def _new_Index(cls, d):
KeyboardInterrupt
16:22:49.821178Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 445, in main
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 336, in train
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 382, in new_sequence_reset
    self.last_read_vector = Variable(torch.Tensor(self.bs, self.W, self.R).zero_().cuda())
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 442, in __setattr__
    object.__setattr__(self, name, value)
KeyboardInterrupt
16:33:00.972316Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 285, in main
    computer, optim, starting_epoch, starting_iteration = load_model(computer)
NameError: name 'load_model' is not defined
16:35:54.599376Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 313, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 229, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 171, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 393, in forward
    hidden_previous_layer)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 531, in forward
    new_hidden = output_gate * torch.tanh(new_state)
KeyboardInterrupt
16:39:01.183053Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 313, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 229, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 182842) exited unexpectedly with exit code 1.
16:49:10.397742Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 202, in run_one_patient
    optimizer.step()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 314, in main
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 230, in train
    printloss=float(train_story_loss[0])
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 202, in run_one_patient
    optimizer.step()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 183073) exited unexpectedly with exit code 1.
16:52:50.997804Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 313, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 229, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 183590) exited unexpectedly with exit code 1.
16:56:05.160919Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 314, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 230, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 172, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 457, in forward
    allocation_weighting = self.allocation_weighting()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 209, in allocation_weighting
    cum_prod = torch.cat([Variable(torch.ones(self.bs, 1).cuda()), cum_prod], 1)[:, :-1]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/_utils.py", line 69, in _cuda
    return new_type(self.size()).copy_(self, async)
KeyboardInterrupt
16:59:00.723522Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 285, in main
    computer, optim, starting_epoch, starting_iteration = load_model(computer)
NameError: name 'load_model' is not defined
17:07:30.194781Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 156, in train_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 302, in main
    target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 192, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 156, in train_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 184413) exited unexpectedly with exit code 1.
17:14:32.303695Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 230, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 172, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 457, in forward
    allocation_weighting = self.allocation_weighting()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 209, in allocation_weighting
    cum_prod = torch.cat([Variable(torch.ones(self.bs, 1).cuda()), cum_prod], 1)[:, :-1]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/_utils.py", line 69, in _cuda
    return new_type(self.size()).copy_(self, async)
KeyboardInterrupt
17:17:07.376152Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 202, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 230, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 202, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 185285) exited unexpectedly with exit code 1.
17:22:10.498217Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 202, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 230, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 202, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 185493) exited unexpectedly with exit code 1.
17:22:49.977928Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 280, in main
    ig = InputGen()
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 45, in __init__
    self.load_pickle(verbose=verbose)
  File "/local2/tmp/pycharm_project_292/death/post/qdata.py", line 172, in load_pickle
    self.serv, self.surg, self.vital = pickle.load(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 178, in _new_Index
    return cls.__new__(cls, **d)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 227, in __new__
    result._set_levels(levels, copy=copy, validate=False)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 304, in _set_levels
    for lev in levels)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 304, in <genexpr>
    for lev in levels)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/numeric.py", line 70, in _shallow_copy
    **kwargs))
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 500, in _shallow_copy
    attributes = self._get_attributes_dict()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 908, in _get_attributes_dict
    return {k: getattr(self, k, None) for k in self._attributes}
KeyboardInterrupt
17:56:28.974167Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 172, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 485, in forward
    raise ValueError("nan is found.")
ValueError: nan is found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 310, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 230, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 207, in run_one_patient
    global_exception_counter+=1
UnboundLocalError: local variable 'global_exception_counter' referenced before assignment
20:38:33.617105Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 294, in main
    computer, optim, starting_epoch, starting_iteration = load_model(computer, optim, starting_epoch, starting_iteration)
TypeError: 'bool' object is not callable
20:43:37.284078Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    if not validate:
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 310, in main
    train(computer, optimizer, real_criterion, binary_criterion,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 230, in train
    train_story_loss = run_one_patient(computer, input, target, target_dim, optimizer, loss_type,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 201, in run_one_patient
    if not validate:
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 187310) exited unexpectedly with exit code 1.
20:46:01.751833Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 311, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, target_dim, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 231, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 172, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 457, in forward
    allocation_weighting = self.allocation_weighting()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 209, in allocation_weighting
    cum_prod = torch.cat([Variable(torch.ones(self.bs, 1).cuda()), cum_prod], 1)[:, :-1]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/_utils.py", line 69, in _cuda
    return new_type(self.size()).copy_(self, async)
KeyboardInterrupt
14:12:43.717553Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 314, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 267, in train
    save_model(computer, optimizer, epoch, i)
KeyboardInterrupt
14:52:12.819052Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 314, in main
    train(computer, optimizer, real_criterion, binary_criterion,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 235, in train
    if i < iter_per_epoch:
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 172, in run_one_patient
    feeding = input[:, timestep, :]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 393, in forward
    hidden_previous_layer)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 529, in forward
    torch.tanh(self.W_state(semicolon_input))
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 55, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/functional.py", line 833, in linear
    if input.dim() == 2 and bias is not None:
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 65, in __getattr__
    if name in self._fallthrough_methods:
KeyboardInterrupt
15:33:28.690135Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 230, in train
    for i, (input, target, loss_type) in enumerate(train):
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 281, in __next__
    return self._process_next_batch(batch)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _process_next_batch
    raise batch.exc_type(batch.exc_msg)
RuntimeError: Traceback (most recent call last):
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 55, in _worker_loop
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 135, in default_collate
    return [default_collate(samples) for samples in transposed]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 135, in <listcomp>
    return [default_collate(samples) for samples in transposed]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 112, in default_collate
    return torch.stack(batch, 0, out=out)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/functional.py", line 66, in stack
    return torch.cat(inputs, dim, out=out)
RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 133 and 10 in dimension 1 at /pytorch/torch/lib/TH/generic/THTensorMath.c:2897

15:34:21.538316Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 230, in train
    for i, (input, target, loss_type) in enumerate(train):
ValueError: not enough values to unpack (expected 3, got 2)
15:35:29.385395Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 237, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 203, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58
15:36:29.216938Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 237, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 203, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58
15:38:16.755272Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 298, in main
    if commpute is None:
NameError: name 'commpute' is not defined
15:39:36.614024Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 316, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 237, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 203, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58
15:42:49.151775Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 238, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 204, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58
15:45:13.371109Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 238, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 204, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58
15:47:06.158720Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 238, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 204, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58
15:48:27.201323Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 314, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 256, in train
    (input,target,loss_type)=next(valid_iterator)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 259, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 259, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 68, in __getitem__
    return self.mother[self.mapping[index]]
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 54, in __getitem__
    return self.get_by_id(id, debug)
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 239, in get_by_id
    allrows = df.loc[[id]]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexing.py", line 1478, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexing.py", line 1901, in _getitem_axis
    return self._getitem_iterable(key, axis=axis)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexing.py", line 1134, in _getitem_iterable
    key, kind=self.name)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 1995, in _convert_listlike_indexer
    _, indexer = self.reindex(keyarr, level=level)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 2084, in reindex
    keep_order=False)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 3953, in _join_level
    ngroups = 1 + new_lev_labels.max()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/numpy/core/_methods.py", line 26, in _amax
    return umr_maximum(a, axis, None, out, keepdims)
KeyboardInterrupt
15:52:36.014205Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 298, in main
    computer, optim, starting_epoch, starting_iteration = load_model(optim, starting_epoch, starting_iteration)
TypeError: load_model() missing 1 required positional argument: 'starting_iteration'
15:56:13.554349Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerCF.py", line 298, in main
    computer, optim, starting_epoch, starting_iteration = load_model(optim, starting_epoch, starting_iteration)
TypeError: load_model() missing 1 required positional argument: 'starting_iteration'
16:11:22.136448Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 313, in main
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 237, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 215, in run_one_patient
    return patient_loss
UnboundLocalError: local variable 'patient_loss' referenced before assignment
16:12:12.839808Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 312, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 230, in train
    for i, (input, target, loss_type) in enumerate(train):
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 275, in __next__
    idx, batch = self._get_batch()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 254, in _get_batch
    return self.data_queue.get()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/queues.py", line 335, in get
    res = self._reader.recv_bytes()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
16:57:56.345949Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 204, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 238, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 204, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 220507) exited unexpectedly with exit code 1.
16:59:01.163471Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 231, in train
    for i, (input, target, loss_type) in enumerate(train):
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 275, in __next__
    idx, batch = self._get_batch()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 254, in _get_batch
    return self.data_queue.get()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/queues.py", line 335, in get
    res = self._reader.recv_bytes()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
17:10:02.364900Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 174, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 485, in forward
    raise ValueError("nan is found.")
ValueError: nan is found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 238, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 212, in run_one_patient
    raise ValueError("Global exception counter reached 10. Likely the model has nan in weights")
ValueError: Global exception counter reached 10. Likely the model has nan in weights
17:36:53.078191Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 174, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 485, in forward
    raise ValueError("nan is found.")
ValueError: nan is found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 238, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 212, in run_one_patient
    raise ValueError("Global exception counter reached 10. Likely the model has nan in weights")
ValueError: Global exception counter reached 10. Likely the model has nan in weights
19:30:25.526896Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 270, in train
    save_model(computer, optimizer, epoch, i)
KeyboardInterrupt
19:32:08.136962Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 270, in train
    save_model(computer, optimizer, epoch, i)
KeyboardInterrupt
19:33:13.509728Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 315, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 270, in train
    save_model(computer, optimizer, epoch, i)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 32, in save_model
    torch.save((net,  optim, epoch, iteration), pickle_file)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 135, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 117, in _with_file_like
    return body(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 135, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 204, in _save
    serialized_storages[key]._write_file(f)
KeyboardInterrupt
20:13:07.672302Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 321, in main
    traindl, iter(validdl), int(starting_epoch) + 1, total_epochs,int(starting_iteration)+1, iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 244, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 176, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 457, in forward
    allocation_weighting = self.allocation_weighting()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 209, in allocation_weighting
    cum_prod = torch.cat([Variable(torch.ones(self.bs, 1).cuda()), cum_prod], 1)[:, :-1]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/_utils.py", line 69, in _cuda
    return new_type(self.size()).copy_(self, async)
KeyboardInterrupt
23:15:56.940120Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 177, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 485, in forward
    raise ValueError("nan is found.")
ValueError: nan is found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 322, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 245, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 219, in run_one_patient
    raise ValueError("Global exception counter reached 10. Likely the model has nan in weights")
ValueError: Global exception counter reached 10. Likely the model has nan in weights
23:52:34.062040Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 207, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 322, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 245, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 207, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 233042) exited unexpectedly with exit code 1.
00:35:35.189324Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 207, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 322, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 245, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 207, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 233087) exited unexpectedly with exit code 1.
04:02:46.185987Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 177, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 485, in forward
    raise ValueError("nan is found.")
ValueError: nan is found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 322, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 245, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 219, in run_one_patient
    raise ValueError("Global exception counter reached 10. Likely the model has nan in weights")
ValueError: Global exception counter reached 10. Likely the model has nan in weights
11:34:45.873878Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 293, in main
    ig = InputGenD()
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 34, in __init__
    super(InputGenD, self).__init__(load_pickle=load_pickle, verbose=verbose, debug=debug)
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 45, in __init__
    self.load_pickle(verbose=verbose)
  File "/local2/tmp/pycharm_project_292/death/post/qdata.py", line 172, in load_pickle
    self.serv, self.surg, self.vital = pickle.load(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 169, in _new_Index
    def _new_Index(cls, d):
KeyboardInterrupt
11:36:58.492519Traceback (most recent call last):
  File "run.py", line 10, in <module>
    main()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 293, in main
    ig = InputGenD()
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 34, in __init__
    super(InputGenD, self).__init__(load_pickle=load_pickle, verbose=verbose, debug=debug)
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 45, in __init__
    self.load_pickle(verbose=verbose)
  File "/local2/tmp/pycharm_project_292/death/post/qdata.py", line 172, in load_pickle
    self.serv, self.surg, self.vital = pickle.load(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 169, in _new_Index
    def _new_Index(cls, d):
KeyboardInterrupt
11:37:51.365552Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 278, in forevermain
    main(True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 293, in main
    ig = InputGenD()
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 34, in __init__
    super(InputGenD, self).__init__(load_pickle=load_pickle, verbose=verbose, debug=debug)
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 45, in __init__
    self.load_pickle(verbose=verbose)
  File "/local2/tmp/pycharm_project_292/death/post/qdata.py", line 172, in load_pickle
    self.serv, self.surg, self.vital = pickle.load(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 169, in _new_Index
    def _new_Index(cls, d):
KeyboardInterrupt
11:39:02.298051Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain()
NameError: name 'forevermain' is not defined
11:40:29.656287Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 285, in forevermain
    main(True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 300, in main
    ig = InputGenD()
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 34, in __init__
    super(InputGenD, self).__init__(load_pickle=load_pickle, verbose=verbose, debug=debug)
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 45, in __init__
    self.load_pickle(verbose=verbose)
  File "/local2/tmp/pycharm_project_292/death/post/qdata.py", line 172, in load_pickle
    self.serv, self.surg, self.vital = pickle.load(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 169, in _new_Index
    def _new_Index(cls, d):
KeyboardInterrupt
11:42:10.387185Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 286, in forevermain
    main(True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 301, in main
    ig = InputGenD()
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 34, in __init__
    super(InputGenD, self).__init__(load_pickle=load_pickle, verbose=verbose, debug=debug)
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 45, in __init__
    self.load_pickle(verbose=verbose)
  File "/local2/tmp/pycharm_project_292/death/post/qdata.py", line 172, in load_pickle
    self.serv, self.surg, self.vital = pickle.load(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 169, in _new_Index
    def _new_Index(cls, d):
KeyboardInterrupt
11:43:33.823610Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 286, in forevermain
    main(True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 329, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 250, in train
    running_loss_deque.appendleft(printloss)
UnboundLocalError: local variable 'printloss' referenced before assignment
13:02:14.347336Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 288, in forevermain
    main(True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 331, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 238, in train
    for i, (input, target, loss_type) in enumerate(train):
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 275, in __next__
    idx, batch = self._get_batch()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 254, in _get_batch
    return self.data_queue.get()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/queues.py", line 335, in get
    res = self._reader.recv_bytes()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
13:03:36.119394Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 288, in forevermain
    main(True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 331, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 266, in train
    (input,target,loss_type)=next(valid_iterator)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 259, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 259, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 68, in __getitem__
    return self.mother[self.mapping[index]]
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 54, in __getitem__
    return self.get_by_id(id, debug)
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 238, in get_by_id
    if id in df.index:
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 547, in __contains__
    self.get_loc(key)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 2264, in get_loc
    loc = self._get_level_indexer(key, level=0)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 2526, in _get_level_indexer
    elif level > 0 or self.lexsort_depth == 0:
  File "pandas/_libs/properties.pyx", line 36, in pandas._libs.properties.CachedProperty.__get__
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 1251, in lexsort_depth
    int64_labels = [_ensure_int64(lab) for lab in self.labels]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 1251, in <listcomp>
    int64_labels = [_ensure_int64(lab) for lab in self.labels]
KeyboardInterrupt
13:04:58.987318Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 288, in forevermain
    main(True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 331, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 266, in train
    (input,target,loss_type)=next(valid_iterator)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 259, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 259, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 68, in __getitem__
    return self.mother[self.mapping[index]]
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 54, in __getitem__
    return self.get_by_id(id, debug)
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 239, in get_by_id
    allrows = df.loc[[id]]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexing.py", line 1478, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexing.py", line 1901, in _getitem_axis
    return self._getitem_iterable(key, axis=axis)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexing.py", line 1134, in _getitem_iterable
    key, kind=self.name)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 1995, in _convert_listlike_indexer
    _, indexer = self.reindex(keyarr, level=level)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 2084, in reindex
    keep_order=False)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 3936, in _join_level
    allow_fill=False)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/algorithms.py", line 1656, in take_nd
    func(arr, indexer, out, fill_value)
KeyboardInterrupt
14:11:58.145095Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 288, in forevermain
    main(True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 331, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 245, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 177, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 457, in forward
    allocation_weighting = self.allocation_weighting()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein.py", line 209, in allocation_weighting
    cum_prod = torch.cat([Variable(torch.ones(self.bs, 1).cuda()), cum_prod], 1)[:, :-1]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/_utils.py", line 69, in _cuda
    return new_type(self.size()).copy_(self, async)
KeyboardInterrupt
17:58:39.556192Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 295, in forevermain
    main(True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 339, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 245, in train
    for i, (input, target, loss_type) in enumerate(train):
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 275, in __next__
    idx, batch = self._get_batch()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 254, in _get_batch
    return self.data_queue.get()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/queues.py", line 335, in get
    res = self._reader.recv_bytes()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
22:52:06.968302Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 210, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 295, in forevermain
    main(load)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 339, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 252, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD.py", line 210, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 45349) exited unexpectedly with exit code 1.
22:52:07.103034Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(False)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 300, in forevermain
    main(load)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 344, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 257, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 185, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 243, in forward
    allocation_weighting = self.allocation_weighting()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 406, in allocation_weighting
    cum_prod = torch.cat([Variable(torch.ones(self.bs, 1).cuda()), cum_prod], 1)[:, :-1]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/_utils.py", line 69, in _cuda
    return new_type(self.size()).copy_(self, async)
KeyboardInterrupt
22:58:50.845894Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(False, 1e-4)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    main(load, lr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 279, in train
    real_criterion, binary_criterion, validate=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 184, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 243, in forward
    allocation_weighting = self.allocation_weighting()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 406, in allocation_weighting
    cum_prod = torch.cat([Variable(torch.ones(self.bs, 1).cuda()), cum_prod], 1)[:, :-1]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/_utils.py", line 69, in _cuda
    return new_type(self.size()).copy_(self, async)
KeyboardInterrupt
22:58:51.242852Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    main(load, lr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 256, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 184, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 176, in forward
    hidden_previous_layer)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 603, in forward
    semicolon_input = torch.cat([input_x, previous_time, previous_layer], dim=1)
KeyboardInterrupt
23:03:33.195815Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    main(load, lr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, savestr, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 256, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 184, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 243, in forward
    allocation_weighting = self.allocation_weighting()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 406, in allocation_weighting
    cum_prod = torch.cat([Variable(torch.ones(self.bs, 1).cuda()), cum_prod], 1)[:, :-1]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/_utils.py", line 69, in _cuda
    return new_type(self.size()).copy_(self, async)
KeyboardInterrupt
23:03:33.825999Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(False, 1e-4, savestr="1e4")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    main(load, lr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, savestr, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 290, in train
    save_model(computer, optimizer, epoch, i, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 38, in save_model
    torch.save((net,  optim, epoch, iteration), pickle_file)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 135, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 117, in _with_file_like
    return body(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 135, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/serialization.py", line 204, in _save
    serialized_storages[key]._write_file(f)
KeyboardInterrupt
23:19:19.535444Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    main(load, lr, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, savestr, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 256, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 184, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 176, in forward
    hidden_previous_layer)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 607, in forward
    forget_gate = torch.sigmoid(self.W_forget(semicolon_input))
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 55, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/functional.py", line 835, in linear
    return torch.addmm(bias, input, weight.t())
KeyboardInterrupt
23:19:19.962100Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(False, 1e-4, savestr="1e4")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    main(load, lr, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, savestr, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 256, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 184, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 243, in forward
    allocation_weighting = self.allocation_weighting()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 406, in allocation_weighting
    cum_prod = torch.cat([Variable(torch.ones(self.bs, 1).cuda()), cum_prod], 1)[:, :-1]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/_utils.py", line 69, in _cuda
    return new_type(self.size()).copy_(self, async)
KeyboardInterrupt
03:56:21.915802Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    main(load, lr, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, savestr, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 249, in train
    for i, (input, target, loss_type) in enumerate(train):
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 281, in __next__
    return self._process_next_batch(batch)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _process_next_batch
    raise batch.exc_type(batch.exc_msg)
KeyError: 'Traceback (most recent call last):\n  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 55, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 55, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 68, in __getitem__\n    return self.mother[self.mapping[index]]\n  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 54, in __getitem__\n    return self.get_by_id(id, debug)\n  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 217, in get_by_id\n    idx = dic[code]\nKeyError: nan\n'
11:12:16.766865Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(False, 1e-4, savestr="1e4")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    main(load, lr, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, savestr, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 256, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 184, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 246, in forward
    allocation_gate, write_gate, allocation_weighting)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 430, in write_weighting
    test_simplex_bound(write_weighting, 1)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 29, in test_simplex_bound
    if (t.sum(1) - 1 > 1e-6).any() or (t.sum(1) < -1e-6).any() or (t < 0).any() or (t > 1).any():
KeyboardInterrupt
11:23:23.771479Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    main(load, lr, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 321, in main
    computer=DNC()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 65, in __init__
    self.RNN_list.append(RNN_Unit(self.x, self.R, self.W, self.h, self.bs))
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 595, in __init__
    self.W_forget = nn.Linear(self.x + self.R * self.W + 2 * self.h, self.h)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 46, in __init__
    self.reset_parameters()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 50, in reset_parameters
    self.weight.data.uniform_(-stdv, stdv)
KeyboardInterrupt
11:23:32.305084Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(False, 1e-4, savestr="1e4")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    main(load, lr, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, savestr, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 277, in train
    (input,target,loss_type)=next(valid_iterator)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 259, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 259, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 68, in __getitem__
    return self.mother[self.mapping[index]]
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 54, in __getitem__
    return self.get_by_id(id, debug)
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 239, in get_by_id
    allrows = df.loc[[id]]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexing.py", line 1478, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexing.py", line 1901, in _getitem_axis
    return self._getitem_iterable(key, axis=axis)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexing.py", line 1134, in _getitem_iterable
    key, kind=self.name)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 1995, in _convert_listlike_indexer
    _, indexer = self.reindex(keyarr, level=level)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 2084, in reindex
    keep_order=False)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 3936, in _join_level
    allow_fill=False)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/algorithms.py", line 1656, in take_nd
    func(arr, indexer, out, fill_value)
KeyboardInterrupt
11:23:53.219712Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    main(load, lr, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, savestr, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 249, in train
    for i, (input, target, loss_type) in enumerate(train):
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 275, in __next__
    idx, batch = self._get_batch()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 254, in _get_batch
    return self.data_queue.get()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/queues.py", line 335, in get
    res = self._reader.recv_bytes()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
11:23:55.397816Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(False, 1e-4, savestr="1e4")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    main(load, lr, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 321, in main
    computer=DNC()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 65, in __init__
    self.RNN_list.append(RNN_Unit(self.x, self.R, self.W, self.h, self.bs))
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 599, in __init__
    self.old_state = Parameter(torch.Tensor(self.bs, self.h).zero_().cuda(),requires_grad=False)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/_utils.py", line 69, in _cuda
    return new_type(self.size()).copy_(self, async)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/cuda/__init__.py", line 387, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
KeyboardInterrupt
11:29:05.711222Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    try:
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 256, in train
    train_story_loss = run_one_patient(computer, input, target, target_dim, optimizer, loss_type,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 184, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 259, in forward
    read_vector = Parameter(self.read_memory(read_weightings).data,requires_grad=False)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 560, in read_memory
    return torch.matmul(self.memory.t(), read_weightings)
KeyboardInterrupt
11:29:07.892633Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 214, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(False, 1e-4, savestr="1e4")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    try:
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 256, in train
    train_story_loss = run_one_patient(computer, input, target, target_dim, optimizer, loss_type,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 214, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 81511) exited unexpectedly with exit code 1.
11:31:00.928740Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 300, in forevermain
    main(load, lr, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 342, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, savestr, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 257, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 184, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 246, in forward
    allocation_gate, write_gate, allocation_weighting)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 436, in write_weighting
    test_simplex_bound(write_weighting, 1)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 29, in test_simplex_bound
    if (t.sum(1) - 1 > 1e-6).any() or (t.sum(1) < -1e-6).any() or (t < 0).any() or (t > 1).any():
KeyboardInterrupt
11:31:01.895787Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 214, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(False, 1e-4, savestr="1e4")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 300, in forevermain
    main(load, lr, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 342, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, savestr, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 257, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 214, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 81765) exited unexpectedly with exit code 1.
11:31:56.091997Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 300, in forevermain
    except ValueError:
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 342, in main
    train(computer, optimizer, real_criterion, binary_criterion,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 291, in train
    print("model saved for epoch", epoch, "input", i)
KeyboardInterrupt
11:32:31.935219Traceback (most recent call last):
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 547, in __contains__
    self.get_loc(key)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 2264, in get_loc
    loc = self._get_level_indexer(key, level=0)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 2526, in _get_level_indexer
    elif level > 0 or self.lexsort_depth == 0:
  File "pandas/_libs/properties.pyx", line 36, in pandas._libs.properties.CachedProperty.__get__
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 1253, in lexsort_depth
    if libalgos.is_lexsorted(int64_labels[:k]):
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    main(load, lr, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, savestr, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 277, in train
    (input,target,loss_type)=next(valid_iterator)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 259, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 259, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 68, in __getitem__
    return self.mother[self.mapping[index]]
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 54, in __getitem__
    return self.get_by_id(id, debug)
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 238, in get_by_id
    if id in df.index:
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/multi.py", line 547, in __contains__
    self.get_loc(key)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 81945) exited unexpectedly with exit code 1.
12:03:33.024663Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 184, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 246, in forward
    allocation_gate, write_gate, allocation_weighting)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 432, in write_weighting
    content_weighting = self.write_content_weighting(write_key, write_strength)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 313, in write_content_weighting
    raise ValueError("NA found in write content weighting")
ValueError: NA found in write content weighting

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(False, 1e-4, savestr="1e4")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    main(load, lr, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, savestr, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 256, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 225, in run_one_patient
    save_model(computer,optimizer,epoch=0,iteration=np.random.randint(0,1000))
TypeError: save_model() missing 1 required positional argument: 'savestr'
14:09:46.458199Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 184, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 246, in forward
    allocation_gate, write_gate, allocation_weighting)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 432, in write_weighting
    content_weighting = self.write_content_weighting(write_key, write_strength)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 313, in write_content_weighting
    raise ValueError("NA found in write content weighting")
ValueError: NA found in write content weighting

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    main(load, lr, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, savestr, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 256, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 225, in run_one_patient
    save_model(computer,optimizer,epoch=0,iteration=np.random.randint(0,1000),savestr="NA")
TypeError: save_model() missing 1 required positional argument: 'savestr'
14:16:18.723854Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(True, 1e-3, savestr="1e3", reset=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 300, in forevermain
    main(load, lr, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 344, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs,int(starting_iteration), iter_per_epoch, savestr, logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 262, in train
    computer.new_sequence_reset()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 154, in new_sequence_reset
    if self.reset:
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 398, in __getattr__
    type(self).__name__, name))
AttributeError: 'Frankenstein' object has no attribute 'reset'
14:43:37.050378Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3", reset=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 300, in forevermain
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 315, in main
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 34, in __init__
    super(InputGenD, self).__init__(load_pickle=load_pickle, verbose=verbose, debug=debug)
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 45, in __init__
    self.load_pickle(verbose=verbose)
  File "/local2/tmp/pycharm_project_292/death/post/qdata.py", line 172, in load_pickle
    self.serv, self.surg, self.vital = pickle.load(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 169, in _new_Index
    def _new_Index(cls, d):
KeyboardInterrupt
15:05:44.524835Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(False, 1e-4, savestr="1e4")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 299, in forevermain
    (i, printloss))
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 343, in main
    if optim is None:
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 290, in train
    val_loss = run_one_patient(computer, input, target, target_dim, optimizer, loss_type,
KeyboardInterrupt
15:28:31.595157Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(False, 1e-4, savestr="1e4")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 312, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 358, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 268, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 193, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 254, in forward
    # read modes [R,3]
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 443, in write_weighting
    '''
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 323, in write_content_weighting
    memnorm = torch.norm(self.memory, 2, 1)
TypeError: file must have a 'write' attribute
17:15:07.699810Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 223, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(False, 1e-4, savestr="1e4",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 312, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 358, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 268, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 223, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 92826) exited unexpectedly with exit code 1.
18:46:23.622819Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 193, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 250, in forward
    write_gate = interface_input[:, last_index:last_index + 1]
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 436, in write_weighting
    ret = torch.gather(allocation_weighting, 1, indices)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 317, in write_content_weighting
    # write_key will be (bs, W)
ValueError: NA found in write content weighting

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3", reset=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 312, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 358, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 268, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 237, in run_one_patient
    pickle.dump(input,save_dir.open("wb"))
TypeError: file must have a 'write' attribute
00:17:54.130148Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(False, 1e-3, savestr="pal",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 312, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 358, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 261, in train
    for i, (input, target, loss_type) in enumerate(train):
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 281, in __next__
    return self._process_next_batch(batch)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 301, in _process_next_batch
    raise batch.exc_type(batch.exc_msg)
KeyError: 'Traceback (most recent call last):\n  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 55, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 55, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 68, in __getitem__\n    return self.mother[self.mapping[index]]\n  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 54, in __getitem__\n    return self.get_by_id(id, debug)\n  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 217, in get_by_id\n    idx = dic[code]\nKeyError: nan\n'
10:06:38.782180Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3", reset=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 313, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 359, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 274, in train
    computer.new_sequence_reset()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 161, in new_sequence_reset
    if self.palette:
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 398, in __getattr__
    type(self).__name__, name))
AttributeError: 'Frankenstein' object has no attribute 'palette'
10:08:20.694413Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3", reset=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 313, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 359, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 274, in train
    computer.new_sequence_reset()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 161, in new_sequence_reset
    if self.palette:
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 398, in __getattr__
    type(self).__name__, name))
AttributeError: 'Frankenstein' object has no attribute 'palette'
10:09:41.871554Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3", reset=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 313, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 359, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 274, in train
    computer.new_sequence_reset()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 161, in new_sequence_reset
    if self.palette:
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 398, in __getattr__
    type(self).__name__, name))
AttributeError: 'Frankenstein' object has no attribute 'palette'
10:12:52.231545Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3", reset=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 313, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 328, in main
    ig = InputGenD()
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 34, in __init__
    super(InputGenD, self).__init__(load_pickle=load_pickle, verbose=verbose, debug=debug)
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 45, in __init__
    self.load_pickle(verbose=verbose)
  File "/local2/tmp/pycharm_project_292/death/post/qdata.py", line 172, in load_pickle
    self.serv, self.surg, self.vital = pickle.load(f)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 169, in _new_Index
    def _new_Index(cls, d):
KeyboardInterrupt
10:14:43.593535Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3", reset=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 317, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 363, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 266, in train
    for i, (input, target, loss_type) in enumerate(train):
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 275, in __next__
    idx, batch = self._get_batch()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 254, in _get_batch
    return self.data_queue.get()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/queues.py", line 335, in get
    res = self._reader.recv_bytes()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
10:17:09.077777Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 227, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3", reset=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 317, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 363, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 273, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 227, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 155419) exited unexpectedly with exit code 1.
10:17:11.304898Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(False, 1e-3, savestr="pal",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 313, in forevermain
    def forevermain(load=False, lr=1e-3, savestr="", reset=True, palette=False):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 359, in main
    # starting with the epoch after the loaded one
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 269, in train
    target_dim = target.shape[2]
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 193, in run_one_patient
    patient_output = Variable(torch.Tensor(1, time_length, target_dim)).cuda()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 196, in forward
    hidden_previous_layer)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 635, in forward
    input_gate = torch.sigmoid(self.W_input(semicolon_input))
KeyboardInterrupt
10:17:46.304276Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(True, 1e-3, savestr="1e3", reset=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 317, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 345, in main
    starting_iteration, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 69, in load_model
    epoch = str(child).split("_")[3]
IndexError: list index out of range
10:17:49.459608Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(True, 1e-3, savestr="pal",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 317, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 345, in main
    starting_iteration, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 69, in load_model
    epoch = str(child).split("_")[3]
IndexError: list index out of range
10:21:19.707158Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(True, 1e-3, savestr="pal",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 320, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 348, in main
    starting_iteration, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 87, in load_model
    with pickle_file.open('rb') as pickle_file:
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/pathlib.py", line 1181, in open
    opener=self._opener)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/pathlib.py", line 1035, in _opener
    return self._accessor.open(self, flags, mode)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/pathlib.py", line 387, in wrapped
    return strfunc(str(pathobj), *args)
FileNotFoundError: [Errno 2] No such file or directory: '/local2/tmp/pycharm_project_292/death/DNC/saves/DNCpal_0_2700.pkl'
10:29:08.610593Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run2.py", line 12, in <module>
    forevermain(True, 1e-3, savestr="pal",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 264, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 292, in main
    starting_iteration, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 73, in load_model
    with pickle_file.open('rb') as pickle_file:
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/pathlib.py", line 1181, in open
    opener=self._opener)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/pathlib.py", line 1035, in _opener
    return self._accessor.open(self, flags, mode)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/pathlib.py", line 387, in wrapped
    return strfunc(str(pathobj), *args)
FileNotFoundError: [Errno 2] No such file or directory: '/local2/tmp/pycharm_project_292/death/DNC/saves/pal/DNC_0_2400.pkl'
10:30:39.760202Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run2.py", line 12, in <module>
    forevermain(True, 1e-3, savestr="pal",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 264, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 310, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 220, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 174, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58
10:31:30.429255Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(True, 1e-3, savestr="1e3", reset=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 264, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 310, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 225, in train
    computer.new_sequence_reset()
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 161, in new_sequence_reset
    if self.palette:
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 398, in __getattr__
    type(self).__name__, name))
AttributeError: 'Frankenstein' object has no attribute 'palette'
10:34:43.307528Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 174, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run.py", line 10, in <module>
    forevermain(False, 1e-3, savestr="1e3", reset=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 264, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 310, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 220, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 174, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 155713) exited unexpectedly with exit code 1.
10:54:31.159302Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 174, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run2.py", line 12, in <module>
    forevermain(True, 1e-3, savestr="pal",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 264, in forevermain
    print("Will run main() forever in a loop.")
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 310, in main
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 220, in train
    train_story_loss = run_one_patient(computer, input, target, target_dim, optimizer, loss_type,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 174, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 155678) exited unexpectedly with exit code 1.
15:52:53.815725Traceback (most recent call last):
  File "run4.py", line 13, in <module>
    forevermain(True, 1e-3, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 269, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 297, in main
    starting_iteration, savestr)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 54, in load_model
    for child in save_dir.iterdir():
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/pathlib.py", line 1079, in iterdir
    for name in self._accessor.listdir(self):
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/pathlib.py", line 387, in wrapped
    return strfunc(str(pathobj), *args)
FileNotFoundError: [Errno 2] No such file or directory: '/local2/tmp/pycharm_project_292/death/DNC/saves/loss'
15:54:39.771040Traceback (most recent call last):
  File "run4.py", line 13, in <module>
    forevermain(False, 1e-3, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 269, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 315, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 223, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 176, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 87, in backward
    grad_variables, create_graph = _make_grads(variables, grad_variables, create_graph)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 35, in _make_grads
    raise RuntimeError("grad can be implicitly created only for scalar outputs")
RuntimeError: grad can be implicitly created only for scalar outputs
16:03:02.762773Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run3.py", line 13, in <module>
    forevermain(True, 1e-3, savestr="pal",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 267, in forevermain
    while True:
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 313, in main
    train(computer, optimizer, real_criterion, binary_criterion,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 221, in train
    if i < iter_per_epoch:
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 144, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 195, in forward
    hidden_output = self.RNN_list[i](input_x_t, self.hidden_previous_timestep[:, i, :],
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 78, in __getitem__
    return Index.apply(self, key)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py", line 79, in forward
    @staticmethod
KeyboardInterrupt
16:03:46.144158Traceback (most recent call last):
  File "run4.py", line 13, in <module>
    forevermain(True, 1e-3, savestr="pal",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 269, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 315, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 223, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 177, in run_one_patient
    optimizer.step()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/optim/adam.py", line 69, in step
    exp_avg.mul_(beta1).add_(1 - beta1, grad)
RuntimeError: arguments are located on different GPUs at /pytorch/torch/lib/THC/generated/../generic/THCTensorMathPointwise.cu:269
16:05:58.131017Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 176, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run4.py", line 13, in <module>
    forevermain(True, 1e-3, savestr="pal",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 269, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 315, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 223, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 176, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 194417) exited unexpectedly with exit code 1.
16:06:42.323233Traceback (most recent call last):
  File "run4.py", line 13, in <module>
    forevermain(False, 1e-3, savestr="pal",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 269, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 291, in main
    computer = DNC(reset=reset, palette=palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 72, in __init__
    self.RNN_list.append(RNN_Unit(self.x, self.R, self.W, self.h, self.bs))
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 621, in __init__
    self.old_state = Parameter(torch.Tensor(self.bs, self.h).zero_().cuda(),requires_grad=False)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/_utils.py", line 69, in _cuda
    return new_type(self.size()).copy_(self, async)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/cuda/__init__.py", line 387, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
KeyboardInterrupt
16:07:37.796696Traceback (most recent call last):
  File "run4.py", line 13, in <module>
    forevermain(False, 1e-3, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 269, in forevermain
    traceback.print_exc()
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 315, in main
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 245, in train
    real_criterion, binary_criterion, validate=True)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 259, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 259, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 68, in __getitem__
    return self.mother[self.mapping[index]]
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planD.py", line 54, in __getitem__
    return self.get_by_id(id, debug)
  File "/local2/tmp/pycharm_project_292/death/post/inputgen_planC.py", line 288, in get_by_id
    if (input != input).any():
KeyboardInterrupt
16:10:54.458462Traceback (most recent call last):
  File "run4.py", line 13, in <module>
    forevermain(False, 1e-3, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 267, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 313, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 221, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 142, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 196, in forward
    hidden_previous_layer)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 631, in forward
    semicolon_input = torch.cat([input_x, previous_time, previous_layer], dim=1)
KeyboardInterrupt
16:14:35.041432Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 142, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 266, in forward
    allocation_gate, write_gate, allocation_weighting)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 455, in write_weighting
    content_weighting = self.write_content_weighting(write_key, write_strength)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 336, in write_content_weighting
    raise ValueError("NA found in write content weighting")
ValueError: NA found in write content weighting

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run4.py", line 13, in <module>
    forevermain(False, 1e-3, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 267, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 313, in main
    traindl, iter(validdl), int(starting_epoch), total_epochs, int(starting_iteration), iter_per_epoch, savestr,
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 221, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 189, in run_one_patient
    pickle.dump(input,fhand)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/storage.py", line 31, in __reduce__
    return type(self), (self.tolist(),)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/storage.py", line 42, in tolist
    return [v for v in self]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/storage.py", line 42, in <listcomp>
    return [v for v in self]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/storage.py", line 17, in <lambda>
    return iter(map(lambda i: self[i], _range(self.size())))
KeyboardInterrupt
16:16:35.973371Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 142, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 266, in forward
    allocation_gate, write_gate, allocation_weighting)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 455, in write_weighting
    content_weighting = self.write_content_weighting(write_key, write_strength)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 336, in write_content_weighting
    raise ValueError("NA found in write content weighting")
ValueError: NA found in write content weighting

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run4.py", line 13, in <module>
    forevermain(False, 1e-3, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 267, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 314, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 221, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 189, in run_one_patient
    pickle.dump(input,fhand)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/storage.py", line 31, in __reduce__
    return type(self), (self.tolist(),)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/storage.py", line 42, in tolist
    return [v for v in self]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/storage.py", line 42, in <listcomp>
    return [v for v in self]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/storage.py", line 42, in <listcomp>
    return [v for v in self]
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_trace_dispatch_regular.py", line 41, in trace_dispatch
    if getattr(t, 'pydev_do_not_trace', None):
KeyboardInterrupt
16:21:01.011995Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run4.py", line 13, in <module>
    forevermain(False, 1e-3, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 267, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 314, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 221, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 142, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 266, in forward
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 455, in write_weighting
    '''
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 336, in write_content_weighting
    save_dir = Path(task_dir) / "saves" / "keykey.pkl"
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 336, in write_content_weighting
    save_dir = Path(task_dir) / "saves" / "keykey.pkl"
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 620, in trace_dispatch
    self.do_wait_suspend(thread, frame, event, arg)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 125, in do_wait_suspend
    self._args[0].do_wait_suspend(*args, **kwargs)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/pydevd.py", line 812, in do_wait_suspend
    time.sleep(0.01)
KeyboardInterrupt
16:25:12.121273Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run4.py", line 13, in <module>
    forevermain(False, 1e-3, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 267, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 314, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 221, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 142, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 198, in forward
    raise ValueError("We have NAN in controller output.")
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 198, in forward
    raise ValueError("We have NAN in controller output.")
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 620, in trace_dispatch
    self.do_wait_suspend(thread, frame, event, arg)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/_pydevd_bundle/pydevd_frame.py", line 125, in do_wait_suspend
    self._args[0].do_wait_suspend(*args, **kwargs)
  File "/infodev1/home/m193194/.pycharm_helpers/pydev/pydevd.py", line 812, in do_wait_suspend
    time.sleep(0.01)
KeyboardInterrupt
16:27:51.159794Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run4.py", line 13, in <module>
    forevermain(False, 1e-3, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 267, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 313, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 221, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD2.py", line 144, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 199, in forward
    hidden_this_timestep[:, i, :] = hidden_output
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 87, in __setitem__
    return SetItem.apply(self, key, value)
KeyboardInterrupt
16:33:06.308538Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 174, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run4.py", line 13, in <module>
    forevermain(False, 1e-3, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 267, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 314, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 221, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 174, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 196314) exited unexpectedly with exit code 1.
16:35:27.725735Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 142, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 198, in forward
    raise ValueError("We have NAN in controller output.")
ValueError: We have NAN in controller output.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run4.py", line 13, in <module>
    forevermain(False, 1e-3, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 267, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 314, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 221, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 189, in run_one_patient
    pickle.dump(input,fhand)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/storage.py", line 31, in __reduce__
    return type(self), (self.tolist(),)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/storage.py", line 42, in tolist
    return [v for v in self]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/storage.py", line 42, in <listcomp>
    return [v for v in self]
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/storage.py", line 17, in <lambda>
    return iter(map(lambda i: self[i], _range(self.size())))
KeyboardInterrupt
16:46:37.086242Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 174, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run4.py", line 13, in <module>
    forevermain(False, 1e-3, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 267, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 314, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 221, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 174, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 196819) exited unexpectedly with exit code 1.
16:49:56.030231Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run4.py", line 13, in <module>
    forevermain(False, 1e-3, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 267, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 314, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 221, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 142, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 196, in forward
    hidden_previous_layer)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 633, in forward
    semicolon_input = torch.cat([input_x, previous_time, previous_layer], dim=1)
KeyboardInterrupt
10:52:10.276934Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 174, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/run4.py", line 13, in <module>
    forevermain(False, 1e-3, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 267, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 314, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 221, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 174, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 220863) exited unexpectedly with exit code 1.
13:23:52.816989Traceback (most recent call last):
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 174, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/variable.py", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run4.py", line 13, in <module>
    forevermain(False, 1e-3, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 267, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 314, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 221, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 174, in run_one_patient
    patient_loss.backward()
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 175, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 221744) exited unexpectedly with exit code 1.
14:03:25.997268Traceback (most recent call last):
  File "run4.py", line 16, in <module>
    forevermain(False, 1e-5, savestr="loss",palette=True)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 267, in forevermain
    main(load, lr, savestr, reset, palette)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 314, in main
    logfile)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 221, in train
    real_criterion, binary_criterion)
  File "/local2/tmp/pycharm_project_292/death/DNC/trainerD3.py", line 142, in run_one_patient
    output = computer(feeding)
  File "/home/m193194/.conda/envs/jason2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 275, in forward
    self.update_precedence_weighting(write_weighting)
  File "/local2/tmp/pycharm_project_292/death/DNC/frankenstein2.py", line 477, in update_precedence_weighting
    ((1 - sum_ww).unsqueeze(1) * self.precedence_weighting + write_weighting).data,
KeyboardInterrupt
